Clustering Notes by Topic: Key Research and Approaches

Clustering a personal vault of markdown notes into coherent topic groups is a challenging unsupervised learning task. Recent research offers several strategies to improve clustering quality, speed, and adaptability. Below, we survey relevant papers and techniques to optimize this clustering mechanism, given the constraints of limited data, no labeled examples, dynamic updates, and cost/runtime considerations.

Embedding-Based Topic Clustering vs. Traditional Methods

Traditional topic modeling like Latent Dirichlet Allocation (LDA) has been widely used but comes with limitations. LDA requires choosing a fixed number of topics and uses a bag-of-words representation that ignores word context, often resulting in less interpretable topics ￼. Newer neural topic models leverage semantic embeddings to overcome these issues ￼ ￼. For example, the Embedded Topic Model (ETM) combines LDA with word embeddings to capture richer semantics, though it still needs a predetermined topic count ￼.

Modern approaches recast topic modeling as a clustering problem in embedding space ￼. Notably, Top2Vec (Angelov 2020) uses joint document and word embeddings to discover topics automatically ￼. It finds an optimal number of topics without supervision and even produces a hierarchical topic tree, yielding more informative and interpretable topics than LDA ￼. Each topic is represented by a “topic vector” in the semantic space, and documents are assigned to their nearest topic vector. Top2Vec can also create a topic hierarchy (merging topics to form higher-level themes), which could be useful if the vault covers broad domains ￼.

Another state-of-the-art method is BERTopic (Grootendorst 2022). BERTopic generates document embeddings using transformers (e.g. BERT), then clusters these embeddings to find groups of similar documents ￼. Finally, it uses a class-based TF–IDF procedure to extract representative keywords for each cluster as the topic label ￼. This approach has been shown to produce coherent topics and is competitive with classical models ￼. In practice, BERTopic often employs HDBSCAN (a density-based clustering algorithm) to find clusters without requiring a preset number of clusters. Research highlights that HDBSCAN can automatically find clusters of varying sizes/densities and doesn’t need the number of clusters specified in advance ￼. This property is valuable when you don’t know in advance how many distinct topics exist in the notes.

Contextual embeddings further improve topic clustering. For instance, Angelov and Inkpen (EMNLP 2024) introduce Contextual Top2Vec, which uses BERT-based token embeddings to create hierarchical topics and even segment documents by topics ￼ ￼. Their model outperforms prior approaches on topic coherence metrics ￼. In their overview of related work, they note Top2Vec’s strengths (auto-detecting topic count, hierarchical topics) and also describe BERTopic’s mechanism of clustering BERT embeddings with class-based TF–IDF topic labels ￼ ￼. The key takeaway is that embedding-based clustering is now a proven technique for topic discovery: by encoding notes in a semantic vector space, we can cluster them such that each cluster corresponds to a meaningful topic.

Recommendation: Use a transformer-based embedding model (e.g. Sentence-BERT or similar) to vectorize each note. Then apply an unsupervised clustering algorithm on these vectors to discover topic groupings. This captures semantic similarity far better than keyword overlap alone ￼ ￼, which is important given the notes may contain varied phrasing. Tools like Top2Vec and BERTopic demonstrate that this approach yields coherent, interpretable clusters without requiring training on labeled data.

Efficient and Scalable Clustering Algorithms

With up to ~100,000 notes, scalability and speed are crucial. Clustering in 1–2 minutes is challenging but feasible with the right algorithms and optimizations:
• Dimensionality Reduction: High-dimensional embeddings (e.g. 768-D BERT vectors) can slow down clustering due to the “curse of dimensionality.” A common trick is to reduce the vectors to a lower dimension (while preserving their relative structure) before clustering. For example, UMAP (Uniform Manifold Approximation and Projection) has been used to compress embeddings into a lower-dimensional space, while retaining local and global structure ￼. Angelov et al. (2024) use UMAP to map document embeddings into 5–50 dimensions, then find dense regions in that space ￼. This speeds up subsequent clustering significantly and can even improve cluster detection by denoising the data.
• Efficient Clustering Algorithms: Among clustering methods, K-Means (especially its variants) and HDBSCAN/DBSCAN are popular for large datasets. Mini-Batch K-Means (Sculley 2010) is an optimized version of k-means that processes data in small batches, drastically reducing runtime with minimal loss in quality. K-means requires specifying k (the number of clusters), which might be unknown – but one can run a cluster validity measure or simply overcluster and then merge clusters if needed. On the other hand, HDBSCAN (Hierarchical DBSCAN) automatically determines the number of clusters and can find arbitrarily shaped clusters. As noted above, HDBSCAN is well-suited for text embeddings because it finds clusters of varying density without a fixed k ￼. Research by McInnes et al. (2017) shows HDBSCAN can discover meaningful clusters without a priori k, which is ideal if the vault’s topical diversity is unknown ￼. Many modern topic modeling pipelines (like BERTopic) use HDBSCAN on reduced-dimensional embeddings ￼.
• Two-Stage Clustering for Large Data: An approach found in literature is to perform pre-clustering to break the dataset into smaller chunks, then cluster those clusters. For example, Amelin et al. (2018) proposed a two-step method to handle very large text collections ￼. First, they ran a fast partitioning algorithm (like k-means) to form many small “micro-clusters.” Then they applied hierarchical agglomerative clustering on those micro-clusters to merge them into final topics ￼. This method managed to cluster a large set of books by writing style and even identify authors automatically ￼. The general principle is that dividing the problem (first pass to group similar notes locally, second pass to group those groups) can improve scalability.
• Incremental and Streaming Clustering: If clustering all 100k notes at once is too slow, consider incremental methods. Incremental k-means (updating centroids as new data comes) or single-pass algorithms can handle data streams or batch updates efficiently. Lu et al. (2012) used an incremental clustering algorithm to detect hot topics in news streams; they combined a single-pass approach with k-NN to continuously identify and label emerging topics ￼. Another study applied incremental k-means for organizing Arabic text documents, showing that unsupervised and semi-supervised clustering (with dimensionality reduction) is effective for large corpora ￼. These results suggest that updating clusters on the fly (rather than re-clustering from scratch) is feasible and can save time as the vault grows. For instance, you could assign new notes to the nearest existing cluster centroid if sufficiently similar (otherwise start a new cluster) – a heuristic often used in streaming clustering algorithms ￼. This would keep clusters “evolving” without a full recomputation each time.

Recommendation: To meet the ~1–2 minute runtime, use a combination of the above. For example, you could embed the notes, reduce dimensionality with something like PCA or UMAP, then run Mini-Batch K-Means or HDBSCAN. BERTopic’s author demonstrated that this pipeline (embedding → dimensionality reduction → HDBSCAN) yields good topics quickly ￼. If k is not known, HDBSCAN is attractive ￼; if you prefer control over k, k-means with a silhouette or elbow analysis could work. For 100k documents, Mini-Batch K-Means can easily run within a minute or two on modern hardware. Also, consider a hybrid: do an initial clustering to get, say, 500 micro-clusters, then either merge similar clusters or label them – this is in line with Amelin et al.’s large-scale approach ￼. Keeping track of cluster centroids also enables incremental updates when a few notes are added or removed, avoiding full recomputation each time.

Dynamic Clustering and Evolving Topics

As the vault content changes and as the user interacts (e.g. takes quizzes), the topic clusters should adapt. Incremental hierarchical clustering research is directly relevant here. A 2023 survey by Simeone et al. analyzes numerous algorithms for dynamic text clustering ￼ ￼. They highlight that traditional static clustering algorithms “require the entire database to be available” and aren’t suited for continuously updated data ￼. Incremental clustering algorithms instead update the existing grouping structure with each change, handling additions or deletions of documents gracefully ￼. For example, algorithms like DHC and DHS (Dynamic Hierarchical Clustering) introduced by Gil-García & Pons-Porrata (2010) maintain a graph-based cluster hierarchy that can be updated when new documents arrive ￼ ￼. Their dynamic algorithms produced hierarchies easier to navigate than static methods like standard agglomerative clustering or bisecting k-means ￼.

Key considerations for evolving clusters include:
• Hierarchical Organization: Using a hierarchical clustering (or a topic hierarchy as in Top2Vec) can make it easier to adjust granularity over time. If a cluster becomes too broad, it might be split into sub-clusters; if new notes bridge two clusters, a parent cluster might group them. Hierarchical topic models allow such flexibility ￼ ￼. In Top2Vec, for instance, an initial fine-grained set of topics can be merged hierarchically to any desired number of topics ￼. This means as the user’s knowledge grows or as the vault expands, you can dynamically decide to make topics more fine-grained (splitting a cluster) or more high-level (merging clusters) to improve the quiz experience.
• Order Sensitivity and Consistency: One challenge in incremental clustering is avoiding sensitivity to the order of data arrival. The survey notes that some papers explicitly aim for clustering that is not sensitive to input order ￼. Techniques like maintaining a global similarity graph or periodically rechecking cluster assignments can help ensure that the final clusters remain meaningful no matter the sequence of note additions.
• No Definitive Strategy (Yet): The survey concludes that “the problem of incremental hierarchical text clustering has not been completely addressed, and there is no definitive strategy for this challenge.” ￼. In practice, this means you may need to combine heuristics and domain knowledge. A simple approach for evolving clusters is to use the existing cluster centroids (or topic vectors) as anchors: when notes are edited or added, recompute their embedding and see if they fit into an existing cluster (within some similarity threshold) or if a new topic cluster should be created. Over time, if a cluster grows too large or becomes semantically incoherent, applying a fresh clustering on that subset might split it into tighter topics.
• User Feedback Loop: Since your application involves quizzing the user and getting feedback (through their answers), you have a unique opportunity to refine clusters with human-in-the-loop signals. For instance, if a user consistently struggles with questions from a particular cluster, it might indicate that cluster is mixing multiple concepts and should be split into sub-topics. Conversely, if the user finds questions too easy or repetitive, perhaps two clusters are actually closely related and could be merged. While there isn’t a specific paper on “quiz feedback for clustering,” this idea relates to active learning and semi-supervised clustering. There is research on clustering with constraints – e.g. must-link or cannot-link constraints provided by a human (Wagstaff et al. 2001) – which could be adapted here. Essentially, the user’s performance can implicitly create constraints (notes that shouldn’t be in the same quiz might belong in separate clusters, etc.). Incorporating such feedback iteratively will make the clusters personalized and improve over time.

Recommendation: Design the clustering process to be incremental. Start with an initial clustering of the vault. Then, schedule periodic updates or triggers when the vault changes significantly. For small updates, an efficient strategy is to only re-embed and place the changed/new notes. Use a similarity threshold to decide if a new note joins an existing cluster or forms a new cluster. If a note is edited heavily, consider reassigning it. Monitor cluster sizes and coherence – if a cluster grows very large, try re-clustering its contents to see if it naturally divides into subtopics. Because this is an ongoing process, investing in efficient algorithms (as discussed above) will pay off. The literature shows many options for incremental clustering; you may not need a sophisticated new algorithm, but rather a combination of known techniques (graph-based clusters, online k-means, etc.) tailored to your data and UI needs ￼ ￼.

Refining Clusters with LLMs and Semantic Feedback

Large Language Models can play two roles in clustering: enhancing cluster interpretation and even performing clustering directly (an emerging idea). Given the cost constraints (clustering 100k notes for <$1), using LLMs directly on every note is impractical. However, LLMs can add value by summarizing or refining clusters after the initial clustering pass:
• Cluster Labeling and Summarization: One challenge after clustering is explaining what each cluster represents (to the user, and even to the algorithm for verification). BERTopic addresses this by extracting keywords for each cluster via class-based TF–IDF ￼. You can take this further by using an LLM to generate a short description or quiz questions for each cluster, based on its member notes. A recent approach by Wang et al. (2025) integrates GPT-4 into the topic modeling pipeline ￼. They first cluster document embeddings with HDBSCAN (using OpenAI embeddings plus UMAP for dimensionality reduction), then use GPT-4 to generate and refine topic descriptions for each cluster ￼. The LLM merges similar topics, deduplicates them, and produces human-readable labels. This GPT-4-assisted refinement yielded more coherent and stable topics than both LDA and BERTopic in their case study, and improved downstream task performance ￼. This suggests that after automated clustering, an LLM can help merge or split clusters at a semantic level and provide meaningful labels (e.g. “Topic: Machine Learning – subtopic: Model Optimization”). In your case, you could use an LLM to name the clusters or even to draft quiz questions for each cluster of notes.
• LLM-Driven Clustering: Cutting-edge research is exploring whether clustering can be done entirely by an LLM in an end-to-end fashion. A very recent paper proposes LLM-MemCluster (Zhu et al. 2025) which treats clustering as a fully LLM-native task ￼. The idea is to leverage the LLM’s internal knowledge to group texts without external embedding or clustering algorithms. They introduce a dynamic memory mechanism to give the LLM a stateful memory across multiple prompts, plus a dual-prompt strategy that lets the model reason about the optimal number of clusters ￼. This approach outperformed traditional pipelines on some benchmarks, all while being interpretable (since the LLM can explain its cluster assignments) ￼. Essentially, the LLM iteratively reads the dataset and forms clusters by itself. However, this method is tuning-free but likely inference-intensive – using an LLM on 100k notes would be costly and slow with current technology. It’s an exciting direction showing how LLMs might cluster via reasoning, but not yet practical under a tight budget.
• Cost-Conscious LLM Usage: Given the cost constraint (100k notes clustering under $1), a pragmatic approach is to use LLMs sparingly. For example, you might only call an LLM on a sample of notes or on cluster prototypes. One idea: after initial clustering via embeddings, take the centroid (representative) of each cluster (perhaps a few central sentences or an auto-generated summary of the cluster) and feed that to an LLM like GPT-3.5 to get a concise topic label or a few quiz questions. This way, the LLM is not reading all 100k notes, just a distilled version of each cluster. The cost would then scale with number of clusters, not number of notes. If you have, say, 50 clusters, a short prompt to GPT-3.5 for each cluster label is very cheap. This aligns with the method of using GPT-4 after clustering to refine topic names in the ICLR 2026 study ￼.
• Iterative Refinement with Feedback: As the user answers quizzes, you gather implicit feedback on cluster quality. You can use an LLM in the loop here too. If certain questions are repeatedly answered incorrectly, you might prompt an LLM to analyze those notes and suggest if they should form a separate topic. Conversely, if a user finds questions easy, an LLM could suggest more challenging, nuanced questions, perhaps indicating the cluster could be merged or needs deeper sub-topics. While there’s no direct research on quiz-based cluster refinement, these are extensions of active learning and curriculum learning concepts. The human feedback (quiz performance) is a powerful signal of how well the cluster serves its purpose (helping the user recall information). Leveraging this to tweak clusters will likely involve some heuristic rules or occasional LLM analysis on problematic clusters.

Recommendation: Use LLMs primarily for cluster interpretation and refinement, rather than the heavy lifting of clustering all notes. Employ a pipeline like: embeddings → clustering → LLM labeling. This maximizes the value of the LLM (making the topics understandable and refined) at minimal token cost. The OpenReview study on GPT-4-assisted topic modeling shows this hybrid approach can significantly improve topic quality ￼. Keep an eye on emerging research like LLM-MemCluster if end-to-end LLM clustering becomes more affordable in the future ￼, but for now a pipeline that uses LLMs selectively is the optimal trade-off.

Conclusion

In summary, the optimal solution appears to be a hybrid approach informed by recent research:
• Unsupervised Embeddings: Use transformer-based embeddings to represent notes in a semantic space, enabling clustering that captures true topical similarity ￼ ￼.
• Efficient Clustering: Apply scalable clustering algorithms (like mini-batch KMeans or HDBSCAN) possibly with dimensionality reduction (UMAP/PCA) to handle tens of thousands of notes within a couple of minutes ￼ ￼. Methods like Top2Vec/BERTopic and various incremental algorithms provide blueprints for this step ￼ ￼.
• Dynamic Updates: Implement incremental updating so clusters evolve as the vault changes. Research indicates numerous strategies (graph-based hierarchies, online k-means, etc.) for maintaining clustering in dynamic environments ￼ ￼. While no single method is definitive ￼, combining these techniques with user feedback will guide the clustering to remain relevant.
• LLM Refinement: Finally, use large language models in a targeted way: to label clusters, summarize them, and refine their composition. Studies show GPT-4 can effectively refine topic descriptions post-clustering ￼, and emerging frameworks even let LLMs determine clusters on their own ￼. Even without full LLM-driven clustering, the interpretability and flexibility added by an LLM (especially as the user interacts with the system) can greatly enhance the quiz experience.

By grounding the design in these research-backed approaches, you can create a clustering mechanism that is coherent, scalable, and adaptive. Your product will not only group related notes for generating quiz questions but will continuously improve those groupings as it learns from the user – effectively turning the personal vault into an evolving, well-organized learning map.

Sources:
• Maarten Grootendorst. “BERTopic: Neural topic modeling with a class-based TF-IDF procedure.” arXiv preprint 2203.05794 (2022). ￼ ￼
• Dimo Angelov, et al. “Topic Modeling: Contextual Token Embeddings Are All You Need.” Findings of EMNLP 2024. ￼ ￼ ￼
• Fernando Simeone et al. “Incremental hierarchical text clustering methods: a review.” arXiv 2312.07769 (2023). ￼ ￼
• Amelin et al. (2018). Clustering by hierarchical approach for large data sets. (as summarized in Simeone et al. 2023) ￼
• Lu et al. (2012). Hot topic detection in news via incremental clustering. (summarized in Simeone et al. 2023) ￼
• Wang, X. et al. “A LLM-Refined Dynamic Topic Clustering Framework for Business Forecasts…” (ICLR 2026 submission, 2025). ￼
• Zhu, Yuanjie et al. “LLM-MemCluster: Empowering Large Language Models with Dynamic Memory for Text Clustering.” arXiv 2511.15424 (Nov 2025). ￼
